{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertNg02/AlbertNg02.github.io/blob/master/Lab2_SolvingMDPswithDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import unittest\n",
        "import functools\n",
        "\n",
        "from collections import namedtuple"
      ],
      "metadata": {
        "id": "UMKYxp2KHqAE"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install and load testing libraries\n",
        "####################################\n",
        "\n",
        "# suppresses output generated during package installation\n",
        "%%capture\n",
        "\n",
        "%pip install ipython_unittest;\n",
        "%pip install jupyter_dojo;\n",
        "\n",
        "%load_ext ipython_unittest"
      ],
      "metadata": {
        "id": "XBa0c65EIDKf"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving MDPs with Dynamic Programming\n",
        "\n",
        "In this lab, we will formalize two sequential decision problems as Markov decision processes (MDPs) and solve them using techniques from dynamic programming that are based on the Bellman equations we discussed in class.\n",
        "\n",
        "The lab will walk you through a set of stages, during which you will implement the iterative algorithms described in Chapter 4 of (Sutton & Barto, 2020). These algorithms include,\n",
        "\n",
        "1. Policy Evaluation,\n",
        "2. Policy Improvement, and\n",
        "3. Value Iteration."
      ],
      "metadata": {
        "id": "jvOkKpWLkJ91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Decision Process #1 - 5x5 Grid World"
      ],
      "metadata": {
        "id": "plqzXchHlbcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by defining a Markov decision process (MDP) for a simple, deterministic, grid world problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "92vIcVjdhdq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1X9EmWHzYqfI9TAN8R5AfE5uJIec0EnTG\" width=900 height=400/>"
      ],
      "metadata": {
        "id": "GUFn9cAUoElN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall from chapter 3 of Sutton & Barto that an MDP is defined by a tuple of elements, $\\langle\\mathcal{S}, \\mathcal{A}, p, r, \\gamma\\rangle$.\n",
        "\n",
        "* $\\mathcal{S}$ is the set of all states.\n",
        "* $\\mathcal{A}$ is the set of all actions.\n",
        "* $p$ is function that defines the environment's dynamics.\n",
        "* $r$ is a reward function that gives the immediate rewards for each state transition, and\n",
        "*$\\gamma$ is a discount factor ($0.0 \\le \\gamma \\le 1.0$) that quantifies the value of future rewards."
      ],
      "metadata": {
        "id": "EmBoNHwnncjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define each of these MDP elements for our problem."
      ],
      "metadata": {
        "id": "TcKcg1HnihB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our goal is to populate this data structure\n",
        "MDP = namedtuple('MDP', ['S', 'A', 'p', 'r', 'gamma'])"
      ],
      "metadata": {
        "id": "H4t0uSBagtHM"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### States\n",
        "\n",
        "Let's start by defining the states."
      ],
      "metadata": {
        "id": "jOpRBcyOlZZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grid size\n",
        "n_rows, n_cols = 5, 5\n",
        "\n",
        "# complete list of states\n",
        "states = np.arange(n_rows * n_cols)\n",
        "\n",
        "# convenience aliases used when defining p() and r()\n",
        "s_goal = [9]       # treasure\n",
        "s_death = [12, 19] # pits\n",
        "s_term = np.array([*s_goal, *s_death])"
      ],
      "metadata": {
        "id": "938-YrzYIRn1"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actions"
      ],
      "metadata": {
        "id": "2CbBG4jnlfJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll define the actions."
      ],
      "metadata": {
        "id": "Dh5DBU1rlihR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# complete list of actions\n",
        "actions = N,S,E,W = np.arange(4)\n",
        "\n",
        "# used later for pretty printing\n",
        "action_map=lambda x: ['N', 'S', 'E', 'W'][x]"
      ],
      "metadata": {
        "id": "KDhR9-fMnIkD"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Dynamics: $p(s' | s, a)$\n",
        "\n",
        "Next, we'll specify the environment's dynamics, $p(s'|s,a)$.\n",
        "\n",
        "In general, $p(\\cdot)$ would also be parameterized on the immediate rewards, $r$. However, since this environment's state transitions AND rewards are *deterministic*, we can simplify $p(\\cdot)$ implementation by leaving off $r$ as a parameter. That is, since there is only a single possible immediate reward, $r$, for each combination of $s$, $s'$, and $a$, so don't need it."
      ],
      "metadata": {
        "id": "j3Y0zOdEFyxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make our lives even easier, we will define a helper function `hit_wall()` that we can use to calculate some of the probabilties returned from our implementation of `p()`. Specifically, whenever we hit a wall, the current and subsequent states should be the same with probability 1.0."
      ],
      "metadata": {
        "id": "aIXoYln2mqdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "\n",
        "def hit_wall(s, a):\n",
        "  \"\"\" Returns true if taking an action a in state s would hit a wall.\n",
        "\n",
        "  Args:\n",
        "    s: the state that action a is taken from\n",
        "    a: the action taken\n",
        "\n",
        "  Returns:\n",
        "    True if agent would hit a wall and False otherwise.\n",
        "  \"\"\"\n",
        "  # north:\n",
        "  if a == N and s in [0, 1, 2, 3, 4, 6, 7, 8, 13, 17, 21, 22, 23, 24]:\n",
        "    return True\n",
        "\n",
        "  # south:\n",
        "  if a == S and s in [1, 2, 3, 7, 8, 16, 17, 18, 20, 21, 22, 23, 24]:\n",
        "    return True\n",
        "\n",
        "  # east:\n",
        "  if a == E and s in [4, 8, 10, 11, 14, 15, 24]:\n",
        "    return True\n",
        "\n",
        "  # west:\n",
        "  if a == W and s in [0, 5, 10, 11, 15, 16, 20]:\n",
        "    return True\n",
        "\n",
        "  return False"
      ],
      "metadata": {
        "id": "Mg8Q3GPWOCCD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's implement `p()`."
      ],
      "metadata": {
        "id": "kMsLggHF7jgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# environment dynamics - pay careful attention to the order of the parameters\n",
        "def p(s, s_prime, a):\n",
        "  \"\"\" Returns the probability of moving from one state to another for an action.\n",
        "\n",
        "  The dynamics are deterministic for this environment, so the returned\n",
        "  values are either 1.0 (legal move) or 0.0 (illegal move).\n",
        "\n",
        "  Args:\n",
        "    s_prime: the successor state resulting from the action\n",
        "    s: the state in which the action was taken\n",
        "    a: the action taken\n",
        "\n",
        "  Returns:\n",
        "    the transition probability, p(s'|s,a) in {0.0, 1.0}\n",
        "  \"\"\"\n",
        "\n",
        "  # same state transitions only possible for terminal states and hitting walls\n",
        "  if hit_wall(s, a) or s in s_term:\n",
        "    return 1.0 if s == s_prime else 0.0\n",
        "\n",
        "  # not (hitting walls or terminal state)\n",
        "  else:\n",
        "    if (a == N) and s_prime == s - n_cols:\n",
        "      return 1.0\n",
        "\n",
        "    if (a == S) and s_prime == s + n_cols:\n",
        "      return 1.0\n",
        "\n",
        "    if (a == E) and s_prime == s + 1:\n",
        "      return 1.0\n",
        "\n",
        "    if (a == W) and s_prime == s - 1:\n",
        "      return 1.0\n",
        "\n",
        "  # illegal transition\n",
        "  return 0.0"
      ],
      "metadata": {
        "id": "xNBZIW1zMXCP"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Function: $r(s, a, s')$"
      ],
      "metadata": {
        "id": "N-wVXH88DeY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define our reward function, $r(\\cdot)$.\n",
        "\n",
        "For this problem, we have the following immediate rewards,\n",
        "1. transitioning into a **goal state** (that is, getting treasure) yields a reward of 25.0.\n",
        "2. transitioning into a **pit state** (that is, death) yields a reward of -100.0, and\n",
        "3. transitioning into **any other state** yields a reward of -1.0.\n",
        "\n",
        "That is,\n",
        "\n",
        "* $r_{goal}=25.0$\n",
        "* $r_{death}=-100.0$\n",
        "* $r_{other}=-1.0$\n"
      ],
      "metadata": {
        "id": "u3NvHgcGoEbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def r(s, s_prime, a):\n",
        "  \"\"\" Returns the reward for a single environmental interaction.\n",
        "\n",
        "  Args:\n",
        "    s: the state before the action was taken\n",
        "    s_prime: the state after the action was taken\n",
        "    a: the action taken\n",
        "\n",
        "  Returns:\n",
        "    an immediate reward based on this environmental interaction\n",
        "  \"\"\"\n",
        "  # reward when transitioning into the goal state\n",
        "  r_goal = 25.0\n",
        "\n",
        "  # reward when transitioning into a pit\n",
        "  r_death = -100.0\n",
        "\n",
        "  # reward for all other transitions\n",
        "  r_other = -1.0\n",
        "\n",
        "  # if current state is terminal then all actions generate zero rewards\n",
        "  if s in s_term:\n",
        "    return 0.0\n",
        "\n",
        "  # transition to goal states\n",
        "  if s_prime in s_goal and s != s_prime:\n",
        "    return r_goal\n",
        "\n",
        "  # transition to death states\n",
        "  if s_prime in s_death and s != s_prime:\n",
        "    return r_death\n",
        "\n",
        "  return r_other"
      ],
      "metadata": {
        "id": "O3D_ldgJnKRI"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's gather all of those pieces and define our MDP."
      ],
      "metadata": {
        "id": "FLQhN4b8p6gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp = MDP(states, actions, p, r, 0.9)"
      ],
      "metadata": {
        "id": "pTYnkNhWp5Ev"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go about the business of solving MDPs using dynamic programming!"
      ],
      "metadata": {
        "id": "uCziRJIUqO4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration\n",
        "\n",
        "The Policy Iteration algorithm (see Sutton & Barto, Section 4.2) is a dynamic programming algorithm that applies an iterative update based on the Bellman Expectation Equation (see Sutton & Barto, Figure 3.14) to discover an optimal policy for an MDP.\n",
        "\n",
        "Specifically, the Policy Iteration algorithm involves the repeated application of two operations:\n",
        "\n",
        "1. **Policy Evaluation** to approximate the state-value function $v_k \\approx v_\\pi$.\n",
        "2. **Policy Improvement** to define a new policy, $\\pi_k$, by acting greedily with respect $v_k$.\n",
        "\n",
        "These steps are repeated until both the state-value estimates and the policy converge to the optimal policy ($\\pi_0 → \\pi_1 → \\cdots \\rightarrow \\pi_*$) and its corresponding state-value function ($v_0 \\rightarrow v_1 \\rightarrow \\cdots \\rightarrow v_*$).\n",
        "\n",
        "Note that for a finite MDP (that is, finite number of states and actions) this convergence is guaranteed.\n",
        "\n",
        "We will be implementing each of the steps separately in multiple stages."
      ],
      "metadata": {
        "id": "7ZsSQNNluo76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions for pretty printing outputs\n",
        "##############################################\n",
        "\n",
        "def display_state_values(V, decimals=2):\n",
        "  \"\"\" Pretty prints the state-value for each state.\n",
        "\n",
        "  Args:\n",
        "    V: an array of state-value estimates\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  print(np.round(V.reshape((n_rows, n_cols)), decimals))\n",
        "\n",
        "def display_action_values(Q, decimals=2):\n",
        "  \"\"\" Pretty prints the action-values for each state.\n",
        "\n",
        "  Args:\n",
        "    Q: an array of action-value estimates\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  print(np.round(Q, decimals))\n",
        "\n",
        "\n",
        "def display_policy(policy, action_map, term_string='-'):\n",
        "  \"\"\" Pretty prints the best action per state for a policy (as a string).\n",
        "\n",
        "  Note that terminals states are displayed with the character '-'.\n",
        "\n",
        "  Args:\n",
        "    policy: the policy whose actions will be displayed.\n",
        "    action_map: a function that maps actions to strings\n",
        "    term_string: a string to use for terminal states\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  # selects the best action per state\n",
        "  actions = np.argmax(Q, axis=1)\n",
        "\n",
        "  # map actions onto action strings\n",
        "  s_array = np.array(list(map(action_map, actions)))\n",
        "\n",
        "  # replace actions for terminal states with term string\n",
        "  s_array[s_term] = term_string\n",
        "\n",
        "  # print out values in a n_rows x n_cols grid\n",
        "  print(s_array.reshape((n_rows, n_cols)))"
      ],
      "metadata": {
        "id": "N7CxY2PYGAFM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Evaluation\n",
        "\n",
        "**You should review Section 4.1 in Sutton & Barto before continuing.**\n",
        "\n",
        "Policy Evaluation is a dynamic programming algorithm that estimates the state-values (or action-values) for a policy using the Bellman Expectation Equation.\n",
        "\n",
        "We call policy evaluation a **prediction** algorithm because it is only used to determine the value of a policy without trying to improve that policy.\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "FTLcKIPTmCmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uniform Random Policy"
      ],
      "metadata": {
        "id": "UQWpmnmOOs-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by implementing a function for a uniform random policy. That is, a function that gives equal probability to select each action, regardless of the state."
      ],
      "metadata": {
        "id": "xXncLxKcNHVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_uniform_random_policy(mdp):\n",
        "  def uniform_random_policy(a, s):\n",
        "    \"\"\" Returns the probability of taking action a from state s.\n",
        "\n",
        "    Based on a uniform random policy.\n",
        "\n",
        "    Args:\n",
        "      s: state (0 <= s < len(states))\n",
        "      a: action taken in state (0 <= a < len(actions))\n",
        "\n",
        "    Returns:\n",
        "      the probability of taking action a in state s (0.0 <= p <= 1.0)\n",
        "    \"\"\"\n",
        "    return 1.0/len(mdp.A)\n",
        "  return uniform_random_policy"
      ],
      "metadata": {
        "id": "MZI1mGxQnJ-G"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll implement a function that updates our array of estimates of the state-value function ($V_k$) based on the previous estimates ($V_{k-1}$).\n"
      ],
      "metadata": {
        "id": "UnDHTbtZO1w-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### State-Value Estimates Update Rule"
      ],
      "metadata": {
        "id": "ZHzi0ny0QeYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Implement the update rule used to iteratively improve our estimates of the state-value function!**\n",
        "\n",
        "Hint: Implement the iterative update given in Section 4.1, Equation 4.5."
      ],
      "metadata": {
        "id": "HtDeJBoQJQ6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp"
      ],
      "metadata": {
        "id": "Ufq8BriLsGMS",
        "outputId": "1f25e39e-bb5a-4760-a528-c39fb5c1535e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MDP(S=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24]), A=array([0, 1, 2, 3]), p=<function p at 0x79ebd42d39a0>, r=<function r at 0x79ebd42d2290>, gamma=0.9)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_map(1)"
      ],
      "metadata": {
        "id": "pJ64SRGpsXfK",
        "outputId": "1c32ed29-d3a5-4e9e-aaba-944f0ffde366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'S'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_state_value_estimate(mdp, V_prev, policy):\n",
        "  \"\"\" Returns an updated estimate of the state-values.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    V_prev: an array of state-value estimates from the previous iteration\n",
        "    policy: the policy being evaluated\n",
        "\n",
        "  Returns:\n",
        "    An array containing updated state-values for every state in the MDP.\n",
        "  \"\"\"\n",
        "  S, A, p, r, gamma = mdp\n",
        "  V = np.zeros(len(states))\n",
        "\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "\n",
        "\n",
        "  for s in S:\n",
        "    val = 0\n",
        "    for a in A:\n",
        "        for s_prime in S:\n",
        "            val += policy(a,s) * p(s, s_prime,a) * (r(s, s_prime, a) + gamma*V_prev[s_prime])\n",
        "    V[s] = val\n",
        "\n",
        "\n",
        "  return V"
      ],
      "metadata": {
        "id": "9gkDBMlE9r0O"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "jVoErhjTqaft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bringing it all together!"
      ],
      "metadata": {
        "id": "Px6UAc_rRJhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(mdp, policy, update, V_init=None, theta=1e-5,\n",
        "                      k_max=np.inf, verbose=True, debug=False):\n",
        "  \"\"\" Approximates the state-value function for a policy.\n",
        "\n",
        "  This is an iterative algorithm that terminates when the max absolute difference\n",
        "  in the state-value estimates between iterations is less than theta OR when the\n",
        "  max number of iterations (k_max) is reached.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    policy: the policy being evaluated\n",
        "    update: a function that updates V based on previous estimate\n",
        "    V_init: initial approximation for the state-values\n",
        "    theta: the max abs difference termination threshold\n",
        "    k_max: the max number of iterations\n",
        "    verbose: if True displays summary info per iteration\n",
        "    debug: if True displays the current V estimates per iteration\n",
        "  Returns:\n",
        "    A tuple where the first element is the number of iterations (k) before\n",
        "    termination/convergence and the second element is an array containing\n",
        "    approximate state-values for every state in mdp, and the second element.\n",
        "  \"\"\"\n",
        "  delta = np.inf # change between iterations\n",
        "  k = 0\n",
        "\n",
        "  V_prev = np.zeros(len(mdp.S)) if V_init is None else V_init\n",
        "\n",
        "  while delta > theta and k < k_max:\n",
        "    if verbose:\n",
        "      print(f'beginning iteration {k}; delta: {delta}')\n",
        "\n",
        "    # iteratively updates the state-value estimates\n",
        "    V = update(mdp, V_prev, policy)\n",
        "\n",
        "    if debug:\n",
        "      display_state_values(V)\n",
        "\n",
        "    # determine the max absolute of state-values between this iteration and\n",
        "    # the last iteration\n",
        "    delta = np.amax(np.abs(V - V_prev))\n",
        "\n",
        "    # copy values to separate array (updates not in-place)\n",
        "    V_prev[:] = V\n",
        "\n",
        "    k += 1\n",
        "\n",
        "  return k, V"
      ],
      "metadata": {
        "id": "8UwBcAdkG7T4"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at the first 5 iterations of our Policy Evaluation implementation on the uniform random policy with debugging output enabled.\n",
        "\n",
        "This displays our state-value function estimates after each iteration.\n",
        "\n",
        "Do the results make sense?\n",
        "* Is `delta` decreasing on each iteration?\n",
        "* Are the values of all states updating? (Except for the terminal states!)"
      ],
      "metadata": {
        "id": "oaBApMFKYF3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniform_random_policy = generate_uniform_random_policy(mdp)\n",
        "\n",
        "# runs the policy evaluation algorithm, displaying the state-value\n",
        "# approximations and delta values after each iteration\n",
        "_ = policy_evaluation(mdp, uniform_random_policy,\n",
        "                      update_state_value_estimate,\n",
        "                      k_max=5, debug=True)"
      ],
      "metadata": {
        "id": "DxA0CpQOKT7e",
        "outputId": "4c60071d-7f40-49c1-f9d7-341acf6111f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginning iteration 0; delta: inf\n",
            "[[ -1.    -1.    -1.    -1.     5.5 ]\n",
            " [ -1.    -1.    -1.    -1.     0.  ]\n",
            " [ -1.    -1.     0.   -25.75 -19.25]\n",
            " [ -1.    -1.    -1.   -25.75   0.  ]\n",
            " [ -1.    -1.    -1.    -1.    -1.  ]]\n",
            "beginning iteration 1; delta: 25.75\n",
            "[[ -1.9   -1.9   -1.9   -0.44   7.75]\n",
            " [ -1.9   -1.9   -1.9   -1.9    0.  ]\n",
            " [ -1.9   -1.9    0.   -41.67 -29.38]\n",
            " [ -1.9   -1.9   -7.47 -37.56   0.  ]\n",
            " [ -1.9   -1.9   -1.9   -1.9   -1.9 ]]\n",
            "beginning iteration 2; delta: 15.918750000000003\n",
            "[[ -2.71  -2.71  -2.38   0.12   8.89]\n",
            " [ -2.71  -2.71  -2.71  -2.71   0.  ]\n",
            " [ -2.71  -2.71   0.   -50.19 -35.23]\n",
            " [ -2.71  -3.96 -13.24 -45.26   0.  ]\n",
            " [ -2.71  -2.71  -2.71  -2.71  -2.71]]\n",
            "beginning iteration 3; delta: 8.517656250000002\n",
            "[[ -3.44  -3.36  -2.65   0.52   9.53]\n",
            " [ -3.44  -3.44  -3.44  -3.44   0.  ]\n",
            " [ -3.44  -3.72   0.   -55.15 -38.47]\n",
            " [ -3.44  -6.37 -18.03 -50.2    0.  ]\n",
            " [ -3.44  -3.44  -3.44  -3.44  -3.44]]\n",
            "beginning iteration 4; delta: 4.966312500000001\n",
            "[[ -4.08  -3.89  -2.83   0.78   9.9 ]\n",
            " [ -4.1   -4.16  -4.1   -4.1    0.  ]\n",
            " [ -4.1   -4.88   0.   -58.11 -40.32]\n",
            " [ -4.1   -8.76 -21.84 -53.51   0.  ]\n",
            " [ -4.1   -4.1   -4.1   -4.1   -4.1 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about if we change our MDP so that $\\gamma=0.0$?"
      ],
      "metadata": {
        "id": "WxniI_Amtbvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp_gamma_zero = mdp._replace(gamma=0.0)\n",
        "_ = policy_evaluation(mdp_gamma_zero, uniform_random_policy,\n",
        "                      update_state_value_estimate, k_max=5, debug=True)"
      ],
      "metadata": {
        "id": "B7D0MR-btaMV",
        "outputId": "c6aa21e8-5f58-4703-dd71-4c5835374b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginning iteration 0; delta: inf\n",
            "[[ -1.    -1.    -1.    -1.     5.5 ]\n",
            " [ -1.    -1.    -1.    -1.     0.  ]\n",
            " [ -1.    -1.     0.   -25.75 -19.25]\n",
            " [ -1.    -1.    -1.   -25.75   0.  ]\n",
            " [ -1.    -1.    -1.    -1.    -1.  ]]\n",
            "beginning iteration 1; delta: 25.75\n",
            "[[ -1.    -1.    -1.    -1.     5.5 ]\n",
            " [ -1.    -1.    -1.    -1.     0.  ]\n",
            " [ -1.    -1.     0.   -25.75 -19.25]\n",
            " [ -1.    -1.    -1.   -25.75   0.  ]\n",
            " [ -1.    -1.    -1.    -1.    -1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about if we change our MDP so that $\\gamma=1.0$"
      ],
      "metadata": {
        "id": "i9g2hEUywSLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mdp_gamma_one = mdp._replace(gamma=1.0)\n",
        "_ = policy_evaluation(mdp_gamma_one, uniform_random_policy,\n",
        "                      update_state_value_estimate, k_max=5, debug=True)"
      ],
      "metadata": {
        "id": "97ou2gXYvAz4",
        "outputId": "b22ec063-5656-4a6d-e535-a6c3d155be96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginning iteration 0; delta: inf\n",
            "[[ -1.    -1.    -1.    -1.     5.5 ]\n",
            " [ -1.    -1.    -1.    -1.     0.  ]\n",
            " [ -1.    -1.     0.   -25.75 -19.25]\n",
            " [ -1.    -1.    -1.   -25.75   0.  ]\n",
            " [ -1.    -1.    -1.    -1.    -1.  ]]\n",
            "beginning iteration 1; delta: 25.75\n",
            "[[ -2.    -2.    -2.    -0.38   8.  ]\n",
            " [ -2.    -2.    -2.    -2.     0.  ]\n",
            " [ -2.    -2.     0.   -43.44 -30.5 ]\n",
            " [ -2.    -2.    -8.19 -38.88   0.  ]\n",
            " [ -2.    -2.    -2.    -2.    -2.  ]]\n",
            "beginning iteration 2; delta: 17.6875\n",
            "[[ -3.    -3.    -2.59   0.31   9.41]\n",
            " [ -3.    -3.    -3.    -3.     0.  ]\n",
            " [ -3.    -3.     0.   -53.95 -37.73]\n",
            " [ -3.    -4.55 -15.31 -48.38   0.  ]\n",
            " [ -3.    -3.    -3.    -3.    -3.  ]]\n",
            "beginning iteration 3; delta: 10.515625\n",
            "[[ -4.    -3.9   -2.97   0.86  10.28]\n",
            " [ -4.    -4.    -4.    -4.     0.  ]\n",
            " [ -4.    -4.39   0.   -60.77 -42.17]\n",
            " [ -4.    -7.85 -21.89 -55.16   0.  ]\n",
            " [ -4.    -4.    -4.    -4.    -4.  ]]\n",
            "beginning iteration 4; delta: 6.8125\n",
            "[[ -4.97  -4.69  -3.24   1.26  10.86]\n",
            " [ -5.    -5.1   -5.    -5.     0.  ]\n",
            " [ -5.    -6.16   0.   -65.27 -44.98]\n",
            " [ -5.   -11.49 -27.7  -60.2    0.  ]\n",
            " [ -5.    -5.    -5.    -5.    -5.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you feel confident that the algorithm is working properly, let's execute it *until convergence* on the original mdp ($\\gamma = 0.9$)."
      ],
      "metadata": {
        "id": "eqhx5HbyX4xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k, V = policy_evaluation(mdp, uniform_random_policy, update_state_value_estimate, verbose=False)"
      ],
      "metadata": {
        "id": "1GA8koDwcZ4c"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's do the same for a modified version of the MDP with $\\gamma=1.0$.\n",
        "\n",
        "**Warning: The following cell can take a while to run (approximately 40 seconds).**"
      ],
      "metadata": {
        "id": "-Ci9U6Sa1Gxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_gamma_one, V_gamma_one = policy_evaluation(mdp_gamma_one, uniform_random_policy, update_state_value_estimate, verbose=False)"
      ],
      "metadata": {
        "id": "quyFhFwyyPw9"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'(iterations when gamma = 0.9): {k}\\n(iterations when gamma = 1.0): {k_gamma_one}')"
      ],
      "metadata": {
        "id": "6QkkIi2o3FUd",
        "outputId": "5a9c610a-3eac-43b3-96de-065d9c6badc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(iterations when gamma = 0.9): 110\n",
            "(iterations when gamma = 1.0): 2463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_state_values(V, decimals=5)"
      ],
      "metadata": {
        "id": "RHh1z2iAeLzi",
        "outputId": "33d5aa93-a84e-4f2b-c543-c76df2b1aecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -9.24397  -7.53782  -4.73738   0.40199  10.16445]\n",
            " [-10.61413 -12.55234 -11.45668 -11.00845   0.     ]\n",
            " [-10.31905 -16.72061   0.      -64.00882 -43.42192]\n",
            " [-10.1658  -23.87584 -37.19812 -62.60847   0.     ]\n",
            " [-10.08627 -10.04511 -10.02404 -10.01368 -10.00945]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_state_values(V_gamma_one, decimals=3)"
      ],
      "metadata": {
        "id": "dwovCkBS2x0x",
        "outputId": "dd7315a0-e055-42e1-9387-cdefa2078ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-174.132 -142.105 -106.079  -66.053  -22.026]\n",
            " [-202.158 -198.185 -206.185 -210.184    0.   ]\n",
            " [-230.158 -182.211    0.     -89.609  -55.536]\n",
            " [-254.157 -162.238 -138.265 -110.291    0.   ]\n",
            " [-274.157 -290.157 -302.157 -310.157 -314.157]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice about the state-value functions (`V` and `V_gamma_one`) and the number of iterations until convergence (`k` and `k_gamma_one`) between these two executions of Policy Evaluation?"
      ],
      "metadata": {
        "id": "fhf8wc201wCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**What explains these differences? Write your observations in the text cell below.**"
      ],
      "metadata": {
        "id": "KtysUXJ228L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`<YOUR ANSWER HERE>` TODO"
      ],
      "metadata": {
        "id": "AkLoOsVf2_F-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verifying Our Solution"
      ],
      "metadata": {
        "id": "zv0sup7x4kky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write a function to verify that the state-values are correct.\n"
      ],
      "metadata": {
        "id": "g96ASgp_43D1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Implement a function to verify that the state-value approximations are correct.**\n",
        "\n",
        "*Hint: Read Sutton & Barto, p. 59 carefully.*"
      ],
      "metadata": {
        "id": "GrV2Db6VI0hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_state_values(mdp, V, policy, tolerance=1e-5):\n",
        "  \"\"\" Verifies that V satisfies the Bellman expectation equation.\n",
        "\n",
        "  Args:\n",
        "    V: array estimates of the state-value function v_pi\n",
        "    policy: the policy that was used to calculate V's values\n",
        "    tolerance: absolute tolerance on equality check\n",
        "\n",
        "  Return:\n",
        "    True if all values in V satisfy the Bellman equation; False otherwise.\n",
        "\n",
        "  \"\"\"\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "  S, A, p, r, gamma = mdp\n",
        "\n",
        "  for s in S:\n",
        "     val = 0\n",
        "     for a in A:\n",
        "        for s_prime in S:\n",
        "            if val == 0:\n",
        "                val = policy(a,s) * p(s, s_prime,a) * (r(s, s_prime, a) + gamma* V[s_prime])\n",
        "            if not (abs(val - V[s]) < tolerance):\n",
        "                return False\n",
        "  return True\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DgfIYzxV40V4"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change this line!\n",
        "%%unittest\n",
        "assert verify_state_values(mdp, V, uniform_random_policy)"
      ],
      "metadata": {
        "id": "19-O9MIvKR1j",
        "outputId": "4632d2c0-628c-4dfe-9c34-95bcd167f365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F\n======================================================================\nFAIL: test_1 (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 1, in test_1\nAssertionError: False is not true\n\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_1 (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 1, in test_1\n",
            "AssertionError: False is not true\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warning: The following cell can take a while to run (approximately 40 seconds).**"
      ],
      "metadata": {
        "id": "FXmgugLkAxTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%unittest_main\n",
        "class TestPolicyEvaluation(unittest.TestCase):\n",
        "\n",
        "    # precision threshold\n",
        "    abs_threshold = 1e-5\n",
        "\n",
        "    def test_gamma_zero(self):\n",
        "      mdp_gamma_zero = mdp._replace(gamma=0.0)\n",
        "\n",
        "      V_expected = np.array(\n",
        "          [-1.0,    -1.0,    -1.0,    -1.0,     5.5,\n",
        "           -1.0,    -1.0,    -1.0,    -1.0,     0.0,\n",
        "           -1.0,    -1.0,     0.0,   -25.75, -19.25,\n",
        "           -1.0,    -1.0,    -1.0,   -25.75,   0.0,\n",
        "           -1.0,    -1.0,    -1.0,    -1.0,    -1.0]\n",
        "          )\n",
        "\n",
        "      _, V_actual = policy_evaluation(mdp_gamma_zero,\n",
        "                                      uniform_random_policy,\n",
        "                                      update_state_value_estimate,\n",
        "                                      theta = TestPolicyEvaluation.abs_threshold,\n",
        "                                      verbose=False)\n",
        "\n",
        "      np.testing.assert_almost_equal(V_expected, V_actual, decimal=3)\n",
        "\n",
        "    def test_gamma_one(self):\n",
        "      mdp_gamma_one = mdp._replace(gamma=1.0)\n",
        "\n",
        "      V_expected = np.array(\n",
        "          [-174.132, -142.105, -106.079,  -66.053,  -22.026,\n",
        "           -202.158, -198.185, -206.185, -210.184,      0.0,\n",
        "           -230.158, -182.211,      0.0,  -89.609,  -55.536,\n",
        "           -254.157, -162.238, -138.265, -110.291,      0.0,\n",
        "           -274.157, -290.157, -302.157, -310.157, -314.157]\n",
        "          )\n",
        "\n",
        "      _, V_actual = policy_evaluation(mdp_gamma_one,\n",
        "                                      uniform_random_policy,\n",
        "                                      update_state_value_estimate,\n",
        "                                      theta = TestPolicyEvaluation.abs_threshold,\n",
        "                                      verbose=False)\n",
        "\n",
        "      np.testing.assert_almost_equal(V_expected, V_actual, decimal=3)\n",
        "\n",
        "    def test_gamma_orig(self):\n",
        "      V_expected = np.array(\n",
        "          [-9.244,  -7.538,  -4.737,   0.402,  10.164,\n",
        "          -10.614, -12.552, -11.457, -11.008,     0.0,\n",
        "          -10.319, -16.721,     0.0, -64.009, -43.422,\n",
        "          -10.166, -23.876, -37.198, -62.608,     0.0,\n",
        "          -10.086, -10.045, -10.024, -10.014, -10.009]\n",
        "          )\n",
        "\n",
        "      _, V_actual = policy_evaluation(mdp,\n",
        "                                      uniform_random_policy,\n",
        "                                      update_state_value_estimate,\n",
        "                                      theta = TestPolicyEvaluation.abs_threshold,\n",
        "                                      verbose=False)\n",
        "\n",
        "      np.testing.assert_almost_equal(V_expected, V_actual, decimal=3)"
      ],
      "metadata": {
        "id": "KGc15xUyczYc",
        "outputId": "43a896a4-f3b4-4d08-de36-d76a4735eb05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "FFF\n======================================================================\nFAIL: test_gamma_one (__main__.TestPolicyEvaluation)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 42, in test_gamma_one\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not almost equal to 3 decimals\n\nMismatched elements: 22 / 25 (88%)\nMax absolute difference: 280.59482233\nMax relative difference: 18.75216651\n x: array([-174.132, -142.105, -106.079,  -66.053,  -22.026, -202.158,\n       -198.185, -206.185, -210.184,    0.   , -230.158, -182.211,\n          0.   ,  -89.609,  -55.536, -254.157, -162.238, -138.265,...\n y: array([ -8.816, -15.966, -18.783, -11.609,  -1.477, -17.297, -33.266,\n       -44.556, -23.176,   0.   , -21.631, -51.243,   0.   , -58.54 ,\n       -43.882, -39.986, -47.074, -61.627, -64.104,   0.   , -43.356,\n       -31.44 , -32.329, -33.249, -33.562])\n\n======================================================================\nFAIL: test_gamma_orig (__main__.TestPolicyEvaluation)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 59, in test_gamma_orig\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not almost equal to 3 decimals\n\nMismatched elements: 22 / 25 (88%)\nMax absolute difference: 26.38475132\nMax relative difference: 3.80993901\n x: array([ -9.244,  -7.538,  -4.737,   0.402,  10.164, -10.614, -12.552,\n       -11.457, -11.008,   0.   , -10.319, -16.721,   0.   , -64.009,\n       -43.422, -10.166, -23.876, -37.198, -62.608,   0.   , -10.086,\n       -10.045, -10.024, -10.014, -10.009])\n y: array([ -4.866,  -9.911, -12.877,  -6.671,   2.113,  -9.493, -22.971,\n       -37.315, -15.553,   0.   , -12.021, -40.931,   0.   , -49.584,\n       -36.582, -27.449, -32.477, -50.207, -53.792,   0.   , -36.471,\n       -21.31 , -22.43 , -24.839, -30.839])\n\n======================================================================\nFAIL: test_gamma_zero (__main__.TestPolicyEvaluation)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 23, in test_gamma_zero\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not almost equal to 3 decimals\n\nMismatched elements: 15 / 25 (60%)\nMax absolute difference: 24.75\nMax relative difference: 1.18181818\n x: array([ -1.  ,  -1.  ,  -1.  ,  -1.  ,   5.5 ,  -1.  ,  -1.  ,  -1.  ,\n        -1.  ,   0.  ,  -1.  ,  -1.  ,   0.  , -25.75, -19.25,  -1.  ,\n        -1.  ,  -1.  , -25.75,   0.  ,  -1.  ,  -1.  ,  -1.  ,  -1.  ,\n        -1.  ])\n y: array([ -0.5 ,  -0.75,  -0.75,  -0.75,   5.75,  -1.  ,  -1.  , -25.75,\n         5.5 ,   0.  ,   5.5 , -25.75,   0.  , -25.75, -19.25,  -1.  ,\n        -1.  , -25.75, -25.75,   0.  , -25.5 ,  -0.75,  -0.75,  -0.75,\n       -25.25])\n\n----------------------------------------------------------------------\nRan 3 tests in 2.486s\n\nFAILED (failures=3)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FFF\n",
            "======================================================================\n",
            "FAIL: test_gamma_one (__main__.TestPolicyEvaluation)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 42, in test_gamma_one\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n",
            "    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n",
            "    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n",
            "    raise AssertionError(msg)\n",
            "AssertionError: \n",
            "Arrays are not almost equal to 3 decimals\n",
            "\n",
            "Mismatched elements: 22 / 25 (88%)\n",
            "Max absolute difference: 280.59482233\n",
            "Max relative difference: 18.75216651\n",
            " x: array([-174.132, -142.105, -106.079,  -66.053,  -22.026, -202.158,\n",
            "       -198.185, -206.185, -210.184,    0.   , -230.158, -182.211,\n",
            "          0.   ,  -89.609,  -55.536, -254.157, -162.238, -138.265,...\n",
            " y: array([ -8.816, -15.966, -18.783, -11.609,  -1.477, -17.297, -33.266,\n",
            "       -44.556, -23.176,   0.   , -21.631, -51.243,   0.   , -58.54 ,\n",
            "       -43.882, -39.986, -47.074, -61.627, -64.104,   0.   , -43.356,\n",
            "       -31.44 , -32.329, -33.249, -33.562])\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_gamma_orig (__main__.TestPolicyEvaluation)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 59, in test_gamma_orig\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n",
            "    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n",
            "    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n",
            "    raise AssertionError(msg)\n",
            "AssertionError: \n",
            "Arrays are not almost equal to 3 decimals\n",
            "\n",
            "Mismatched elements: 22 / 25 (88%)\n",
            "Max absolute difference: 26.38475132\n",
            "Max relative difference: 3.80993901\n",
            " x: array([ -9.244,  -7.538,  -4.737,   0.402,  10.164, -10.614, -12.552,\n",
            "       -11.457, -11.008,   0.   , -10.319, -16.721,   0.   , -64.009,\n",
            "       -43.422, -10.166, -23.876, -37.198, -62.608,   0.   , -10.086,\n",
            "       -10.045, -10.024, -10.014, -10.009])\n",
            " y: array([ -4.866,  -9.911, -12.877,  -6.671,   2.113,  -9.493, -22.971,\n",
            "       -37.315, -15.553,   0.   , -12.021, -40.931,   0.   , -49.584,\n",
            "       -36.582, -27.449, -32.477, -50.207, -53.792,   0.   , -36.471,\n",
            "       -21.31 , -22.43 , -24.839, -30.839])\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_gamma_zero (__main__.TestPolicyEvaluation)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 23, in test_gamma_zero\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n",
            "    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n",
            "    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n",
            "    raise AssertionError(msg)\n",
            "AssertionError: \n",
            "Arrays are not almost equal to 3 decimals\n",
            "\n",
            "Mismatched elements: 15 / 25 (60%)\n",
            "Max absolute difference: 24.75\n",
            "Max relative difference: 1.18181818\n",
            " x: array([ -1.  ,  -1.  ,  -1.  ,  -1.  ,   5.5 ,  -1.  ,  -1.  ,  -1.  ,\n",
            "        -1.  ,   0.  ,  -1.  ,  -1.  ,   0.  , -25.75, -19.25,  -1.  ,\n",
            "        -1.  ,  -1.  , -25.75,   0.  ,  -1.  ,  -1.  ,  -1.  ,  -1.  ,\n",
            "        -1.  ])\n",
            " y: array([ -0.5 ,  -0.75,  -0.75,  -0.75,   5.75,  -1.  ,  -1.  , -25.75,\n",
            "         5.5 ,   0.  ,   5.5 , -25.75,   0.  , -25.75, -19.25,  -1.  ,\n",
            "        -1.  , -25.75, -25.75,   0.  , -25.5 ,  -0.75,  -0.75,  -0.75,\n",
            "       -25.25])\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 2.486s\n",
            "\n",
            "FAILED (failures=3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=3 errors=0 failures=3>"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Improvement\n",
        "\n",
        "In this section you will implement the Policy Improvement step of the Policy Iteration algorithm.\n",
        "\n",
        "We will do this in stages.\n",
        "\n",
        "1. Calculate the action-value estimates, $Q \\approx q_\\pi(s,a)$, from the state-value estimates, $V \\approx v_\\pi(s)$.\n",
        "2. Define a greedy policy based on $Q$.\n",
        "3. Identify whether two policies are \"stable\" (the convergence criterion)"
      ],
      "metadata": {
        "id": "5qY9q-B5l7ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimating action-values ($Q$) from $V$"
      ],
      "metadata": {
        "id": "7f3sP8pPKj8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Calculate the array of the action-value estimates ($Q$) from the array of state-value estimates ($V$)!**\n",
        "\n",
        "Hint: See Sutton & Barto, p. 79, Equation 4.9."
      ],
      "metadata": {
        "id": "c734jeX7hSDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_Q(mdp, V):\n",
        "  \"\"\" Calculates an array of action-values from state-value estimates.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    V: an array of state-value estimates\n",
        "\n",
        "  Return:\n",
        "    An array of action-value estimates.\n",
        "  \"\"\"\n",
        "  S, A, p, r, gamma = mdp\n",
        "  Q = np.zeros(shape=(len(states), len(actions)))\n",
        "\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "#################################\n",
        "\n",
        "  for s in S:\n",
        "    val = 0\n",
        "    for a in A:\n",
        "        for s_prime in S:\n",
        "            Q[s][a] += p(s, s_prime, a) * (r(s,s_prime, a) + gamma * V[s_prime])\n",
        "\n",
        "\n",
        "\n",
        "#   for s in S:\n",
        "#     val = 0\n",
        "#     for a in A:\n",
        "#         for s_prime in S:\n",
        "#             val += policy(a,s) * p(s, s_prime,a) * (r(s, s_prime, a) + gamma*V_prev[s_prime])\n",
        "#     V[s] = val\n",
        "\n",
        "\n",
        "  return Q"
      ],
      "metadata": {
        "id": "WzDt48nzITJI"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdp"
      ],
      "metadata": {
        "id": "NCGHaGNtOacJ",
        "outputId": "209a7487-bcba-4535-e02d-0dd9a44a321b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MDP(S=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24]), A=array([0, 1, 2, 3]), p=<function p at 0x79ebd42d39a0>, r=<function r at 0x79ebd42d2290>, gamma=0.9)"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = calc_Q(mdp, V)\n",
        "Q"
      ],
      "metadata": {
        "id": "DT-z1NuoyWfn",
        "outputId": "d1baf381-2221-46bc-ad07-dc5680b428b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0.,   -1.,   -1.,    0.],\n",
              "       [   0.,   -1.,   -1.,   -1.],\n",
              "       [   0.,   -1.,   -1.,   -1.],\n",
              "       [   0.,   -1.,   -1.,   -1.],\n",
              "       [   0.,   25.,   -1.,   -1.],\n",
              "       [  -1.,   -1.,   -1.,   -1.],\n",
              "       [  -1.,   -1.,   -1.,   -1.],\n",
              "       [  -1., -100.,   -1.,   -1.],\n",
              "       [  -1.,   -1.,   25.,   -1.],\n",
              "       [   0.,    0.,    0.,    0.],\n",
              "       [  -1.,   -1.,   -1.,   25.],\n",
              "       [  -1.,   -1., -100.,   -1.],\n",
              "       [   0.,    0.,    0.,    0.],\n",
              "       [  -1.,   -1.,   -1., -100.],\n",
              "       [  25., -100.,   -1.,   -1.],\n",
              "       [  -1.,   -1.,   -1.,   -1.],\n",
              "       [  -1.,   -1.,   -1.,   -1.],\n",
              "       [-100.,   -1.,   -1.,   -1.],\n",
              "       [  -1.,   -1., -100.,   -1.],\n",
              "       [   0.,    0.,    0.,    0.],\n",
              "       [  -1.,    0.,   -1., -100.],\n",
              "       [  -1.,    0.,   -1.,   -1.],\n",
              "       [  -1.,    0.,   -1.,   -1.],\n",
              "       [  -1.,    0.,   -1.,   -1.],\n",
              "       [-100.,    0.,    0.,   -1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%unittest_testcase\n",
        "def test_calcQ(self):\n",
        "  V_test = np.array(\n",
        "      [-1.0,    -1.0,    -1.0,    -1.0,     5.5,\n",
        "        -1.0,    -1.0,    -1.0,    -1.0,     0.0,\n",
        "        -1.0,    -1.0,     0.0,   -25.75, -19.25,\n",
        "        -1.0,    -1.0,    -1.0,   -25.75,   0.0,\n",
        "        -1.0,    -1.0,    -1.0,    -1.0,    -1.0]\n",
        "      )\n",
        "\n",
        "  Q_expected = np.array(\n",
        "     [[  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,    3.95 ,   -1.9  ],\n",
        "      [   3.95 ,   25.0  ,    3.95 ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [   0.0  ,    0.0  ,    0.0  ,    0.0  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [   0.0  ,    0.0  ,    0.0  ,    0.0  ],\n",
        "      [ -24.175,  -24.175,  -18.325, -100.0  ],\n",
        "      [  25.0  , -100.0  ,  -18.325,  -24.175],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,  -24.175,   -1.9  ],\n",
        "      [ -24.175,  -24.175, -100.0  ,   -1.9  ],\n",
        "      [   0.0  ,    0.0  ,    0.0  ,    0.0  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
        "      [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ]]\n",
        "      )\n",
        "\n",
        "  Q_actual = calc_Q(mdp, V_test)\n",
        "\n",
        "  np.testing.assert_almost_equal(Q_expected, Q_actual, decimal=3)"
      ],
      "metadata": {
        "id": "iZAbZesUDIyl",
        "outputId": "cf146af5-58f7-4643-db39-d574168b47b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F\n======================================================================\nFAIL: test_calcQ (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 40, in test_calcQ\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not almost equal to 3 decimals\n\nMismatched elements: 27 / 100 (27%)\nMax absolute difference: 98.1\nMax relative difference: 11.72368421\n x: array([[  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n       [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n       [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],...\n y: array([[   0.   ,   -1.9  ,   -1.9  ,    0.   ],\n       [   0.   ,   -1.9  ,   -1.9  ,   -1.9  ],\n       [   0.   ,   -1.9  ,   -1.9  ,   -1.9  ],...\n\n----------------------------------------------------------------------\nRan 1 test in 0.027s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_calcQ (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 40, in test_calcQ\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 521, in assert_almost_equal\n",
            "    return assert_array_almost_equal(actual, desired, decimal, err_msg)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 1034, in assert_array_almost_equal\n",
            "    assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n",
            "    raise AssertionError(msg)\n",
            "AssertionError: \n",
            "Arrays are not almost equal to 3 decimals\n",
            "\n",
            "Mismatched elements: 27 / 100 (27%)\n",
            "Max absolute difference: 98.1\n",
            "Max relative difference: 11.72368421\n",
            " x: array([[  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
            "       [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
            "       [  -1.9  ,   -1.9  ,   -1.9  ,   -1.9  ],...\n",
            " y: array([[   0.   ,   -1.9  ,   -1.9  ,    0.   ],\n",
            "       [   0.   ,   -1.9  ,   -1.9  ,   -1.9  ],\n",
            "       [   0.   ,   -1.9  ,   -1.9  ,   -1.9  ],...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.027s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating a Greedy Policy from $Q$"
      ],
      "metadata": {
        "id": "pTs1NU7sLCz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Generate a greedy policy based on Q!**"
      ],
      "metadata": {
        "id": "NZw2AI6Jh5N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_greedy_policy(Q):\n",
        "  \"\"\" Returns a new policy based on greedy selection over Q.\n",
        "\n",
        "  Args:\n",
        "    Q: array estimate of the action-value function\n",
        "  Returns:\n",
        "    a new greedy policy function\n",
        "  \"\"\"\n",
        "  def greedy_policy(s, a):\n",
        "    #################################\n",
        "    # Add your implementation here! #\n",
        "    #################################\n",
        "    pass\n",
        "\n",
        "  return greedy_policy"
      ],
      "metadata": {
        "id": "fJbpdIaqJj5j"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_policy = generate_greedy_policy(Q)"
      ],
      "metadata": {
        "id": "n86UL-jl8xiG"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display our greedy policy. Does it make sense given the $Q$ values we calculated earlier?"
      ],
      "metadata": {
        "id": "1tQLYb1XCZri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_policy(greedy_policy, action_map=lambda x: ['N', 'S', 'E', 'W'][x])"
      ],
      "metadata": {
        "id": "HH1xw8oH9g8H",
        "outputId": "cf706011-e5df-41f4-eb5e-22c624b5f8da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['N' 'N' 'N' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' '-' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' 'N' 'N' 'N']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_state_values(V, decimals=3)"
      ],
      "metadata": {
        "id": "5wmJXNBtmp91",
        "outputId": "03a49afa-9dce-4729-ed36-b0e0557e80ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -9.244  -7.538  -4.737   0.402  10.164]\n",
            " [-10.614 -12.552 -11.457 -11.008   0.   ]\n",
            " [-10.319 -16.721   0.    -64.009 -43.422]\n",
            " [-10.166 -23.876 -37.198 -62.608   0.   ]\n",
            " [-10.086 -10.045 -10.024 -10.014 -10.009]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_action_values(Q, decimals=10)"
      ],
      "metadata": {
        "id": "spTJDxsmCr1k",
        "outputId": "4ed538bb-43c5-4bb4-b9aa-90573549e8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%unittest_testcase\n",
        "def test_generate_greedy_policy(self):\n",
        "\n",
        "  Q_test = np.array(\n",
        "    [[  -9.0,  -11.0,   -8.0,   -9.0],\n",
        "      [  -8.0,   -8.0,   -5.0,   -9.0],\n",
        "      [  -5.0,   -5.0,   -1.0,   -8.0],\n",
        "      [  -1.0,   -1.0,    8.0,   -5.0],\n",
        "      [   8.0,   25.0,    8.0,   -1.0],\n",
        "      [  -9.0,  -10.0,  -12.0,  -11.0],\n",
        "      [ -12.0,  -16.0,  -11.0,  -11.0],\n",
        "      [ -11.0,  -11.0,  -11.0,  -12.0]]\n",
        "  )\n",
        "\n",
        "  greedy_policy = generate_greedy_policy(Q)\n",
        "\n",
        "  expected = np.eye(len(mdp.A))[[2, 2, 2, 2, 1, 0, 3, 2]]\n",
        "  actual = np.array([[greedy_policy(s, a) for a in mdp.A] for s in range(Q_test.shape[0])])\n",
        "\n",
        "  np.testing.assert_equal(expected, actual)"
      ],
      "metadata": {
        "id": "4Ek7iU3fTTcH",
        "outputId": "82d7c182-d4c1-47f0-a581-139a0e552670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F\n======================================================================\nFAIL: test_generate_greedy_policy (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 19, in test_generate_greedy_policy\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 282, in assert_equal\n    return assert_array_equal(actual, desired, err_msg, verbose)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 920, in assert_array_equal\n    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n    return func(*args, **kwds)\n  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nArrays are not equal\n\nMismatched elements: 32 / 32 (100%)\n x: array([[0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],...\n y: array([[None, None, None, None],\n       [None, None, None, None],\n       [None, None, None, None],...\n\n----------------------------------------------------------------------\nRan 1 test in 0.002s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_generate_greedy_policy (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 19, in test_generate_greedy_policy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 282, in assert_equal\n",
            "    return assert_array_equal(actual, desired, err_msg, verbose)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 920, in assert_array_equal\n",
            "    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\", line 797, in assert_array_compare\n",
            "    raise AssertionError(msg)\n",
            "AssertionError: \n",
            "Arrays are not equal\n",
            "\n",
            "Mismatched elements: 32 / 32 (100%)\n",
            " x: array([[0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0.],...\n",
            " y: array([[None, None, None, None],\n",
            "       [None, None, None, None],\n",
            "       [None, None, None, None],...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.002s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Identifying \"Stable\" Policies?\n",
        "\n",
        "It can be shown (see Sutton & Barto, 2020, pp. 78-79) that the policies resulting from each iteration of the Policy Iteration algorithm must be strictly better than the policies from the previous iterations unless the policy is optimal.\n",
        "\n",
        "We can use this idea to define our convergence criterion for the Policy Iteration algorithm: if the greedy policies generated between two subsequent iterations of the Policy Iteration algorithm would result in the **same selection of actions for every state**, then those policies must be optimal; therefore, we should terminate the algorithm.\n",
        "\n",
        "When policies fail to improve between iterations they are called **stable** policies."
      ],
      "metadata": {
        "id": "LaosFjWTRW8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Implement a function that determines whether two policies are stable.**"
      ],
      "metadata": {
        "id": "MffW0GmsjFdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_stable(mdp, p1, p2):\n",
        "  \"\"\" Returns whether these two policies are stable.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    p1: 1st policy to compare\n",
        "    p2: 2nd policy to compare\n",
        "\n",
        "  Returns:\n",
        "    True if stable; False otherwise\n",
        "  \"\"\"\n",
        "  S, A, *_ = mdp\n",
        "\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "_ahseCo3Rvz1"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%unittest_testcase\n",
        "def test_stable(self):\n",
        "  maxes = np.random.default_rng().integers(0, len(mdp.A), size=len(mdp.S))\n",
        "  maxes_as_prob = np.eye(len(mdp.A))[maxes]\n",
        "\n",
        "  def policy1(s, a):\n",
        "    return maxes_as_prob[s][a]\n",
        "\n",
        "  def policy2(s, a):\n",
        "    return maxes_as_prob[s][a]\n",
        "\n",
        "  self.assertTrue(is_stable(mdp, policy1, policy2))\n",
        "\n",
        "def test_not_stable(self):\n",
        "  p1_maxes = np.random.default_rng().integers(0, len(mdp.A), size=len(mdp.S))\n",
        "\n",
        "  # copy array and perturb one element so policies are not identical\n",
        "  p2_maxes = np.empty_like(p1_maxes)\n",
        "  p2_maxes[:] = p1_maxes\n",
        "  p2_maxes[5] = (p2_maxes[5] - 1) % len(mdp.A)\n",
        "\n",
        "  def policy1(s, a):\n",
        "    return np.eye(len(mdp.A))[p1_maxes][s][a]\n",
        "\n",
        "  def policy2(s, a):\n",
        "    return np.eye(len(mdp.A))[p2_maxes][s][a]\n",
        "\n",
        "  self.assertFalse(is_stable(mdp, policy1, policy2))"
      ],
      "metadata": {
        "id": "ZM3QgO61Taci",
        "outputId": "514dad15-5fe7-4c84-c03a-5de82d9c491b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F.\n======================================================================\nFAIL: test_not_stable (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 27, in test_not_stable\nAssertionError: True is not false\n\n----------------------------------------------------------------------\nRan 2 tests in 0.002s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F.\n",
            "======================================================================\n",
            "FAIL: test_not_stable (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 27, in test_not_stable\n",
            "AssertionError: True is not false\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.002s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=2 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together!"
      ],
      "metadata": {
        "id": "1zvqzZyYmUOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "1KNjU_JVQQ4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(mdp, V, plcy_prev):\n",
        "  \"\"\" Returns an updated policy based on the last state-value estimates.\n",
        "\n",
        "  Once the improvement step generates a \"stable\" policy, the algorithm has\n",
        "  converged.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    V: initial approximation for the state-values\n",
        "    plcy_prev: the max abs difference termination threshold\n",
        "\n",
        "  Returns:\n",
        "    A tuple where the first element is the updated policy and the second element\n",
        "    is a Boolean indicating whether the new policy is stable.\n",
        "  \"\"\"\n",
        "  Q = calc_Q(mdp, V)\n",
        "  plcy_new = generate_greedy_policy(Q)\n",
        "\n",
        "  return plcy_new, is_stable(mdp, plcy_prev, plcy_new)"
      ],
      "metadata": {
        "id": "Vky4lM5yau-k"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: The implementation in Sutton & Barto (p. 81) assumes a deterministic policy, which simplifies the notation to $\\pi(s)$ instead of $\\pi(a|s)$. This assumption is valid because we are defining a greedy policy and choosing the first action to take in the event of a tie. That said, the algorithm works well for stochastic policies $\\pi(a|s)$ as well, and the only difference between what we are implementing here and the algorithm described in Sutton & Barto is that we need to loop over all actions, whereas Sutton & Barton's algorithm only needs to evaluate a single action per state. In short, so long as all suboptimal actions are given probability 0.0, then the algorithm is guaranteed to converge. (See Sutton & Barto, p. 79.)*"
      ],
      "metadata": {
        "id": "wytoGPjTQIM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(mdp, theta=1e-9, eval_limit=np.inf,\n",
        "                     verbose=True, debug=False):\n",
        "  \"\"\" Returns the optimal policy and state-value function for this MDP.\n",
        "\n",
        "  This is an iterative algorithm that terminates when a stable policy is found.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    theta: the max abs difference termination threshold used for policy eval\n",
        "    eval_limit: the max number of policy eval iterations\n",
        "    verbose: if True displays summary info per iteration\n",
        "    debug: if True displays the current V estimates per iteration\n",
        "  Returns:\n",
        "    A tuple where the first element is the number of iterations,\n",
        "    the second element is an array containing optimal state-values\n",
        "    for every state in mdp, and the third element is the optimal policy.\n",
        "  \"\"\"\n",
        "  k = 0\n",
        "  V = np.zeros(len(mdp.S))\n",
        "\n",
        "  # initial policy is uniform random\n",
        "  policy = generate_uniform_random_policy(mdp)\n",
        "\n",
        "  stable = False\n",
        "  while not stable:\n",
        "    if verbose:\n",
        "      print(f'beginning iteration {k}', flush=True)\n",
        "\n",
        "    _, V = policy_evaluation(mdp, policy, update_state_value_estimate, V,\n",
        "                             theta, eval_limit, verbose=False, debug=False)\n",
        "\n",
        "    if debug:\n",
        "      display_state_values(V)\n",
        "\n",
        "    policy, stable = policy_improvement(mdp, V, policy)\n",
        "\n",
        "    k += 1\n",
        "\n",
        "  return k, V, policy"
      ],
      "metadata": {
        "id": "69xSg-nbKy97"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k, V, policy = policy_iteration(mdp, debug=False)"
      ],
      "metadata": {
        "id": "2tE_zYnHdcPz",
        "outputId": "f5490ccc-6af1-43a1-9dc1-a3782f00ad45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginning iteration 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_state_values(V, 3)"
      ],
      "metadata": {
        "id": "dXAOs97lgYSE",
        "outputId": "df2653ef-2d9b-4247-a6f6-faa00ffd63e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -9.244  -7.538  -4.737   0.402  10.164]\n",
            " [-10.614 -12.552 -11.457 -11.009   0.   ]\n",
            " [-10.319 -16.721   0.    -64.009 -43.422]\n",
            " [-10.166 -23.876 -37.198 -62.608   0.   ]\n",
            " [-10.086 -10.045 -10.024 -10.014 -10.01 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_action_values(calc_Q(mdp, V))"
      ],
      "metadata": {
        "id": "CI9XCRzD0O_A",
        "outputId": "f37fff5b-662e-4e04-d5d6-1f225eec75b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_policy(policy, action_map)"
      ],
      "metadata": {
        "id": "kUdu0xCzgdC-",
        "outputId": "58c9b448-9a65-4719-ceee-ebd8b52803a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['N' 'N' 'N' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' '-' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' 'N' 'N' 'N']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%unittest_testcase\n",
        "def test_policy_iteration(self):\n",
        "  S, A, *_ = mdp\n",
        "  V_expected = np.array(\n",
        "    [12.964,  15.515,  18.35,   21.5,   25.0,\n",
        "    10.667,     8.6,   6.74,  5.066,    0.0,\n",
        "        8.6,  10.667,    0.0,   21.5,   25.0,\n",
        "      6.74,  12.964, 15.515,  18.35,    0.0,\n",
        "      5.066,    3.56,  2.204,  0.983, -0.115])\n",
        "\n",
        "  def policy_expected(s, a):\n",
        "    actions = np.array(\n",
        "        [2, 2, 2, 2, 1,\n",
        "         0, 1, 3, 3, 0,\n",
        "         0, 1, 0, 2, 0,\n",
        "         0, 2, 2, 0, 0,\n",
        "         0, 3, 3, 3, 3])\n",
        "\n",
        "    return np.eye(len(A))[actions][s][a]\n",
        "\n",
        "  k, V_actual, policy_actual = policy_iteration(mdp, verbose=False)\n",
        "\n",
        "  for s in S:\n",
        "    for a in A:\n",
        "      self.assertEqual(policy_actual(s, a), policy_expected(s,a))"
      ],
      "metadata": {
        "id": "G0ILP6nX8DXF",
        "outputId": "f11df4c8-386d-48b0-c115-e474c4371218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F\n======================================================================\nFAIL: test_policy_iteration (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 24, in test_policy_iteration\nAssertionError: None != 0.0\n\n----------------------------------------------------------------------\nRan 1 test in 5.058s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_policy_iteration (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 24, in test_policy_iteration\n",
            "AssertionError: None != 0.0\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 5.058s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verifying Our Solution\n",
        "\n",
        "*Hint: Read Sutton & Barto, Sec. 3.6 carefully.*"
      ],
      "metadata": {
        "id": "8qX9teUNaa_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_optimal_state_values(mdp, V, tolerance=1e-5):\n",
        "  \"\"\" Verify the V for an optimal policy satisfies the Bellman optimality equation.\n",
        "\n",
        "  Args:\n",
        "    V: array estimates for the optimal state-value function\n",
        "    tolerance: absolute tolerance on equality check\n",
        "\n",
        "  Return:\n",
        "    True if all values in V satisfy the Bellman optimality equation; False\n",
        "    otherwise.\n",
        "\n",
        "  \"\"\"\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "eS68FP8nacLv"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change this line!\n",
        "%%unittest\n",
        "assert verify_optimal_state_values(mdp, V)"
      ],
      "metadata": {
        "id": "kjR5rRw6cUBC",
        "outputId": "0f6271be-a73f-495b-89c9-c55c4e241394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F\n======================================================================\nFAIL: test_1 (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 1, in test_1\nAssertionError: None is not true\n\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_1 (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 1, in test_1\n",
            "AssertionError: None is not true\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.000s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration"
      ],
      "metadata": {
        "id": "J6fVI7RAkAw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Waiting for the Policy Evaluation step of the Policy Iteration algorithm to converge before each Policy Improvement step can be extremely time consuming. A key insight (see Sutton & Barto, 2020, Sec. 4.4) is that its not necessary to wait for Policy Evaluation to converge before applying the Policy Improvement step.\n",
        "\n",
        "In the extreme, we can execute a single iteration of Policy Evaluation for each Policy Improvement. This idea is the basis for the **Value Iteration** algorithm."
      ],
      "metadata": {
        "id": "flNq-pfpAcHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Complete the implementation for the Value Iteration algorithm given below.**"
      ],
      "metadata": {
        "id": "YA5d5c75qQOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(mdp, theta=1e-9, verbose=True, debug=False):\n",
        "  \"\"\" Returns the optimal policy and state-value function for this MDP.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    theta: the max abs difference termination threshold used for convergence\n",
        "    verbose: if True displays summary info per iteration\n",
        "    debug: if True displays the current V estimates per iteration\n",
        "  Returns:\n",
        "    A tuple where the first element is the number of iterations,\n",
        "    the second element is an array containing optimal state-values\n",
        "    for every state in mdp, and the third element is the optimal policy.\n",
        "  \"\"\"\n",
        "  S, A, p, r, gamma = mdp\n",
        "  k = 0\n",
        "  delta = np.inf\n",
        "\n",
        "  V_prev = np.zeros(len(S))\n",
        "\n",
        "  while delta > theta:\n",
        "    if verbose:\n",
        "      print(f'beginning iteration {k}; delta: {delta}', flush=True)\n",
        "\n",
        "    V = np.zeros(len(S))\n",
        "\n",
        "    #################################\n",
        "    # Add your implementation here! #\n",
        "    #################################\n",
        "\n",
        "    if debug:\n",
        "      display_state_values(V, 2)\n",
        "\n",
        "    delta = np.amax(np.abs(V - V_prev))\n",
        "    V_prev[:] = V\n",
        "\n",
        "    k += 1\n",
        "\n",
        "  return k, V, generate_greedy_policy(calc_Q(mdp, V))"
      ],
      "metadata": {
        "id": "sXWvZQwwkFWr"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k, V, policy = value_iteration(mdp)"
      ],
      "metadata": {
        "id": "V_5GD05Jcb4l",
        "outputId": "aec93c97-78ef-4415-b01e-5794957cee20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginning iteration 0; delta: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = calc_Q(mdp, V)\n",
        "display_action_values(Q)"
      ],
      "metadata": {
        "id": "3PrdnnU6wLNT",
        "outputId": "dd9dd4c6-59c2-4038-afa4-cc6db6b5a365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_state_values(V, 3)"
      ],
      "metadata": {
        "id": "iACeZSJ5wbhS",
        "outputId": "ab98c393-1484-411f-e3ae-d24d7e0896f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_policy(policy, action_map)"
      ],
      "metadata": {
        "id": "mzAoVmbqwTb9",
        "outputId": "12ce79f8-6277-4a36-c1cb-58e5ab4e9065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['N' 'N' 'N' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' '-' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' 'N' 'N' 'N']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%unittest_testcase\n",
        "def test_value_iteration(self):\n",
        "  S, A, *_ = mdp\n",
        "  V_expected = np.array(\n",
        "    [12.964,  15.515,  18.35,   21.5,   25.0,\n",
        "    10.667,     8.6,   6.74,  5.066,    0.0,\n",
        "        8.6,  10.667,    0.0,   21.5,   25.0,\n",
        "      6.74,  12.964, 15.515,  18.35,    0.0,\n",
        "      5.066,    3.56,  2.204,  0.983, -0.115])\n",
        "\n",
        "  def policy_expected(s, a):\n",
        "    actions = np.array(\n",
        "        [2, 2, 2, 2, 1,\n",
        "         0, 1, 3, 3, 0,\n",
        "         0, 1, 0, 2, 0,\n",
        "         0, 2, 2, 0, 0,\n",
        "         0, 3, 3, 3, 3])\n",
        "\n",
        "    return np.eye(len(A))[actions][s][a]\n",
        "\n",
        "  k, V_actual, policy_actual = value_iteration(mdp, verbose=False)\n",
        "\n",
        "  for s in S:\n",
        "    for a in A:\n",
        "      self.assertEqual(policy_actual(s, a), policy_expected(s,a))"
      ],
      "metadata": {
        "id": "Ai2XuANqThAR",
        "outputId": "7fbf1c1a-5054-4343-f108-50656d16b584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F\n======================================================================\nFAIL: test_value_iteration (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 24, in test_value_iteration\nAssertionError: None != 0.0\n\n----------------------------------------------------------------------\nRan 1 test in 0.001s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_value_iteration (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 24, in test_value_iteration\n",
            "AssertionError: None != 0.0\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.001s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's compare Value Iteration to Policy Iteration when only a single evaluation step is taken (that is, when `eval_limit=1`."
      ],
      "metadata": {
        "id": "cLQUvTcWnFSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k, V, policy = policy_iteration(mdp, eval_limit=1)"
      ],
      "metadata": {
        "id": "NlVKeUj1nlYd",
        "outputId": "5073217f-17c9-4a8b-a0a0-0b33f95622b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginning iteration 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_policy(policy, action_map)"
      ],
      "metadata": {
        "id": "nBJn6sL_BBAF",
        "outputId": "928b46bd-42cf-4913-a76d-85b154f63a82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['N' 'N' 'N' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' '-' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' 'N' 'N' 'N']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_state_values(V)"
      ],
      "metadata": {
        "id": "-81GvbtdBOE3",
        "outputId": "f02e8f09-2742-4daf-ec1f-55ef0821b18d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ -1.    -1.    -1.    -1.     5.5 ]\n",
            " [ -1.    -1.    -1.    -1.     0.  ]\n",
            " [ -1.    -1.     0.   -25.75 -19.25]\n",
            " [ -1.    -1.    -1.   -25.75   0.  ]\n",
            " [ -1.    -1.    -1.    -1.    -1.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k, V, policy = value_iteration(mdp)"
      ],
      "metadata": {
        "id": "pi6u6s34n6Jf",
        "outputId": "448382a9-438d-4f4a-dfd3-bfb89086afc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beginning iteration 0; delta: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_policy(policy, action_map)"
      ],
      "metadata": {
        "id": "aMFotTaBBEJr",
        "outputId": "5d9efecd-3a98-43be-ebc5-c9712d66efcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['N' 'N' 'N' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' '-' 'N' 'N']\n",
            " ['N' 'N' 'N' 'N' '-']\n",
            " ['N' 'N' 'N' 'N' 'N']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_state_values(V)"
      ],
      "metadata": {
        "id": "HSVujR2RBIr0",
        "outputId": "0d1a5c36-b988-4edd-9dc0-e71fc04e6868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Verifying Our Solution"
      ],
      "metadata": {
        "id": "yuy3duQ1cFsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change this line!\n",
        "%%unittest\n",
        "assert verify_optimal_state_values(mdp, V)"
      ],
      "metadata": {
        "id": "wvK0AzBdcJZ-",
        "outputId": "fd5fe8bd-318c-41e8-8bb8-4e781777a14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "yellow",
              "message": "",
              "previous": 0
            },
            "text/plain": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/unittest.status+json": {
              "color": "salmon",
              "message": "F\n======================================================================\nFAIL: test_1 (__main__.JupyterTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"Cell Tests\", line 1, in test_1\nAssertionError: None is not true\n\n----------------------------------------------------------------------\nRan 1 test in 0.008s\n\nFAILED (failures=1)\n",
              "previous": 0
            },
            "text/plain": [
              "Fail"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "======================================================================\n",
            "FAIL: test_1 (__main__.JupyterTest)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"Cell Tests\", line 1, in test_1\n",
            "AssertionError: None is not true\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.008s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Policy Iteration"
      ],
      "metadata": {
        "id": "79_zCgWADQQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value iteration is the logically the same as Policy Iteration with only single a evaluation step per policy improvement.\n",
        "\n",
        "However, in general, we can choose different limits to the number of evaluation iterations and get different convergence behavior.\n",
        "\n",
        "Play around with the `policy_iteration()` function you implemented above and see how the number of evaluation steps (set using the `eval_limit` parameter) affects the convergence of the algorithm."
      ],
      "metadata": {
        "id": "7ybhvXg8DVYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Policy Improvement Iterations vs. Policy Evaluation Iterations\n",
        "\n",
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n"
      ],
      "metadata": {
        "id": "n5w1yq9BDoW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run `policy_iteration()` multiple times, setting `eval_limit` to each value in the set `{1,2,5,10}`, and then plot 'Policy Evaluation Iterations' vs. 'Policy Improvement Iterations'**"
      ],
      "metadata": {
        "id": "i-SQLARej48X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add code to run policy_iteration where eval_limit = 1,2,5,10"
      ],
      "metadata": {
        "id": "G-BQpB5oETIq"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add code to plot your results"
      ],
      "metadata": {
        "id": "95ug0rg0kX6D"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Markov Decision Process #2 - And Now With Teleporters!\n",
        "\n",
        "Now it's your turn to formalize a sequential decision problem as a Markov Decision Process (MDP)!\n",
        "\n",
        "The problem shown below is an updated $5\\times5$ grid world that has new wall boundaries, new pit locations, and teleporters!\n",
        "\n",
        "Using a teleporter requires that agent's execute a \"USE(TELEPORTER)\" action from a cell containing a teleporter. If this action is taken, then the agent will be wisked away to the teleporter's pair and reappear in that state (see diagram).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1qx3P7Ipqs8r7nxKLh7s4hbE942ljILGP\" width=1200 height=600/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KHR4cfr1T16T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Teleporter Dynamics and Rewards**\n",
        "\n",
        "90% of the time, the teleport will transport the agent to the other teleporter, where you incur the usual $r =-1$ reward for taking an action; *however*, there is a 10% chance that the teleporter will **malfunction**, not only failing to teleport the agent to the destination teleporter (that is, it remains in the same state), but it also causes a painful burning sensation ($r = -10)$!\n",
        "\n",
        "Take note! This is the first **non-deterministic** state transition and reward for our grid world."
      ],
      "metadata": {
        "id": "pckQOtJxm-9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Define the Markov Decision Process (MDP) for the Problem Given Below**\n",
        "\n",
        "This requires you to,\n",
        "\n",
        "1. define the states,\n",
        "2. define the actions,\n",
        "3. define the environment's dynamics by implementing `p()`, and\n",
        "4. define the environment's reward function by implement `r()`."
      ],
      "metadata": {
        "id": "tj3NsfbRgbb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### States"
      ],
      "metadata": {
        "id": "rfPTywJOulQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add code to define the states of the MDP"
      ],
      "metadata": {
        "id": "g9b9m0RJ0TbR"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actions"
      ],
      "metadata": {
        "id": "pTfN0F1gu2Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add code to define the actions of the MDP"
      ],
      "metadata": {
        "id": "9-HjupFV3ot6"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Dynamics: $p(s'|s,a)$"
      ],
      "metadata": {
        "id": "R3B7xq5Zu4J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# update hit_walls function, which will be used to define the environments\n",
        "# dynamics\n",
        "def hit_wall(s, a):\n",
        "  \"\"\" Returns true if taking an action a in state s would hit a wall.\n",
        "\n",
        "  Args:\n",
        "    s: the state that action a is taken from\n",
        "    a: the action taken\n",
        "\n",
        "  Returns:\n",
        "    True if agent would hit a wall and False otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "\n",
        "  # didn't hit a wall\n",
        "  return False"
      ],
      "metadata": {
        "id": "zY98kN5hWwRz"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement the environment dynamics function, $p(s'|s,a)$!**\n",
        "\n",
        "Hints:\n",
        "\n",
        "1. Refer to the MDP we defined in the previous example to guide you.\n",
        "2. Use the `hit_wall()` function you defined above to simplify your implementation of `p()`.\n",
        "3. The environment is now non-deterministic (when using the teleporter), so special care needs to be taken in this case.\n",
        "\n",
        "\n",
        "Note: Even though the environment's dynamics are non-deterministic, we **do not** need to add a parameter $r$ to `p()`. Given $s'$, $s$, and $a$, the probability of receiving a particular $r$ value is still always 1.0. That is,\n",
        "rewards are still deterministic given a particular state transition, even though the state transitions themselves are not."
      ],
      "metadata": {
        "id": "_eNrxsBV3QIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement p(s'|s,a) to define the environment dynamics\n",
        "def p(s, s_prime, a):\n",
        "  \"\"\" Returns the probability of moving from one state to another for an action.\n",
        "\n",
        "  Args:\n",
        "    s_prime: the successor state resulting from the action\n",
        "    s: the state in which the action was taken\n",
        "    a: the action taken\n",
        "\n",
        "  Returns:\n",
        "    the transition probability, p(s'|s,a) in {0.0, 1.0}\n",
        "  \"\"\"\n",
        "\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "QYewPYVd0HTW"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Runction: $r(s, a, s')$"
      ],
      "metadata": {
        "id": "HfOupI1ku-6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement r(s',s,a) to define the environment's reward function\n",
        "def r(s, s_prime, a):\n",
        "  \"\"\" Returns the reward for a single environmental interaction.\n",
        "\n",
        "  Args:\n",
        "    s: the state before the action was taken\n",
        "    s_prime: the state after the action was taken\n",
        "    a: the action taken\n",
        "  Returns:\n",
        "    an immediate reward based on this environmental interaction\n",
        "  \"\"\"\n",
        "\n",
        "  # Add your implementation here!\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "JcNeWgxt0IWZ"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance of the updated MDP\n",
        "mdp = MDP(states, actions, p, r, gamma)"
      ],
      "metadata": {
        "id": "H5UAq7V6v74G",
        "outputId": "d700dc87-c069-45fb-c44c-109ab6ce5cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gamma' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-e2255166ef8d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create an instance of the updated MDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gamma' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "_BEsqPXKlaha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Use the algorithms we implemented in the previous section to solve this MDP!**\n",
        "\n",
        "*Note: A complete solution should include the optimal state-value function and the optimal policy for the MDP.*\n",
        "\n",
        "*You should display both of these using the provided helper functions.*"
      ],
      "metadata": {
        "id": "Wmr6gt6ElU6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add whatever code (and cells) you need to solve the MDP, verify the solution,\n",
        "# and display stuff!"
      ],
      "metadata": {
        "id": "mO13wNeIrbcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Credit\n",
        "\n",
        "In general, performing calculations in Python is very *slow*.\n",
        "\n",
        "We can often dramatically improve the runtime of our algorithms by using matrices and vectors instead of nested loops like we did above. Doing so  allows us to pass a large data set to a library like NumPy, which can then perform a series of vectorized operations on that data (e.g., matrix multiplications) in C."
      ],
      "metadata": {
        "id": "bDIWtOzZilzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function\n",
        "def display_2d_matrix(matrix, fmt=lambda x:x):\n",
        "  \"\"\" Pretty prints a 2D matrix, optimally applying a formatting function.\n",
        "\n",
        "  Args:\n",
        "    matrix: a 2D numpy array\n",
        "    fmt: a function to applied to each cell's value before printing (optional)\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  for row in matrix:\n",
        "    print('\\t'.join([str(fmt(col)) for col in row]))"
      ],
      "metadata": {
        "id": "HCrPNpCX81Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updated MDP using Matrices\n",
        "\n",
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Transform each element of the MDP into a matrix or vector (as appropriate)**"
      ],
      "metadata": {
        "id": "d0AjXdLnfevM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define matrix P\n",
        "P = ..."
      ],
      "metadata": {
        "id": "eUdoEdmZ1KCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define matrix R\n",
        "R = ..."
      ],
      "metadata": {
        "id": "03FDrksE1MxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize your mdp!\n",
        "mdp = MDP(S, A, P, R, gamma)"
      ],
      "metadata": {
        "id": "bO8lkQLz0vq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Evaluation with Vectorized Operations\n",
        "\n",
        "Hints:\n",
        "\n",
        "1. Create a new `update_state_value_estimate_vec_ops()` function and pass that into `policy_evalution()`'s `update` parameter.\n",
        "2. Perform matrix multiplications, dot products, etc. (as needed) inside of `update_state_value_estimate_vec_ops()` to perform the Bellman expectation equation's update.\n",
        "3. Test your solution to make sure you get the same answers as you did with the non-vectorized implementation!"
      ],
      "metadata": {
        "id": "_8w43m2ryI7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Now rewrite the update rule for Policy Evaluation to use vectorized operations.**"
      ],
      "metadata": {
        "id": "cMkEulpUdqnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_state_value_estimate_vec_ops(mdp, V_prev, policy):\n",
        "  \"\"\" Returns an updated estimate of the state-values.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP (in vectorized form)\n",
        "    V_prev: an array of state-value estimates from the previous iteration\n",
        "    policy: the policy being evaluated\n",
        "\n",
        "  Returns:\n",
        "    An array containing updated state-values for every state in the MDP.\n",
        "  \"\"\"\n",
        "\n",
        "  # P and R are now matrices!\n",
        "  S, A, P, R, gamma = mdp\n",
        "\n",
        "  V = np.zeros(len(states))\n",
        "\n",
        "\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "\n",
        "\n",
        "  return V"
      ],
      "metadata": {
        "id": "txB_N5i6ryIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice that the update function that we are passing in is the function with\n",
        "# vectorized operations\n",
        "_ = policy_evaluation(mdp, uniform_random_policy,\n",
        "                      update_state_value_estimate_vec_ops,\n",
        "                      k_max=5, debug=True)"
      ],
      "metadata": {
        "id": "tx5ejRsI0j_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration with Vectorized Operations"
      ],
      "metadata": {
        "id": "FB_No1PBe2Sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, rewrite the Value Iteration algorithm to use vectorized operations.\n",
        "\n",
        "This will require changing two functions\n",
        "1. `calc_Q()`\n",
        "2. `value_iteration()`\n",
        "\n",
        "Stubs for the vectorized versions are provided below."
      ],
      "metadata": {
        "id": "EfNQpAbkd1-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Now rewrite the `calc_Q()` to use vectorized operations.**"
      ],
      "metadata": {
        "id": "dFmY7k5tgefI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_Q_with_vec_ops(mdp, V):\n",
        "  \"\"\" Calculates an array of action-values from state-value estimates.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    V: an array of state-value estimates\n",
        "\n",
        "  Return:\n",
        "    An array of action-value estimates.\n",
        "  \"\"\"\n",
        "  # P and R are now matrices!\n",
        "  S, A, P, R, gamma = mdp\n",
        "\n",
        "  Q = np.zeros(shape=(len(states), len(actions)))\n",
        "\n",
        "  #################################\n",
        "  # Add your implementation here! #\n",
        "  #################################\n",
        "\n",
        "  return Q"
      ],
      "metadata": {
        "id": "TUHvcI-2eZRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Now rewrite the `value_iteration()` to use vectorized operations.**"
      ],
      "metadata": {
        "id": "G1aQGenOglsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration_with_vec_ops(mdp, theta=1e-9, verbose=True, debug=False):\n",
        "  \"\"\" Returns the optimal policy and state-value function for this MDP.\n",
        "\n",
        "  Args:\n",
        "    mdp: the MDP\n",
        "    theta: the max abs difference termination threshold used for convergence\n",
        "    verbose: if True displays summary info per iteration\n",
        "    debug: if True displays the current V estimates per iteration\n",
        "  Returns:\n",
        "    A tuple where the first element is the number of iterations,\n",
        "    the second element is an array containing optimal state-values\n",
        "    for every state in mdp, and the third element is the optimal policy.\n",
        "  \"\"\"\n",
        "  # P and R are now matrices!\n",
        "  S, A, P, R, gamma = mdp\n",
        "  k = 0\n",
        "  delta = np.inf\n",
        "\n",
        "  V_prev = np.zeros(len(S))\n",
        "\n",
        "  while delta > theta:\n",
        "    if verbose:\n",
        "      print(f'beginning iteration {k}; delta: {delta}', flush=True)\n",
        "\n",
        "    V = np.zeros(len(S))\n",
        "\n",
        "    #################################\n",
        "    # Add your implementation here! #\n",
        "    #################################\n",
        "\n",
        "    if debug:\n",
        "      display_state_values(V, 2)\n",
        "\n",
        "    delta = np.amax(np.abs(V - V_prev))\n",
        "    V_prev[:] = V\n",
        "\n",
        "    k += 1\n",
        "\n",
        "  return k, V, generate_greedy_policy(calc_Q_with_vec_ops(mdp, V))"
      ],
      "metadata": {
        "id": "SRflAIRHeDyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Solutions"
      ],
      "metadata": {
        "id": "ncZSKz_Pe7tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Write test cases to verify your new implementations work correctly!**"
      ],
      "metadata": {
        "id": "uckmIiSDfCU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your functional test cases here!"
      ],
      "metadata": {
        "id": "cXat8Okpgxp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Timing Results"
      ],
      "metadata": {
        "id": "y9ew1TRrdEj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use `%%timeit` to compare the performance of your vectorized implementation of `policy_evaluation()` to the earlier, non-vectorized implementation.\n",
        "\n",
        "That is, compare the timing of `policy_evaluation()` using `update=update_state_value_estimate` and `update=update_state_value_estimate_vec_ops`."
      ],
      "metadata": {
        "id": "NaT46alE6o49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://img.freepik.com/premium-vector/scientist-professor-holding-stop-sign_20412-3130.jpg\" width=\"225\" height=\"200\">\n",
        "\n",
        "**Write performance tests!**"
      ],
      "metadata": {
        "id": "VAtuN6Njg_Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your performance tests and results here!"
      ],
      "metadata": {
        "id": "PkEFRLPTvogn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}